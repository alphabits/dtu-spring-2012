\def\assignmenttitle{Written exercise 2}
\def\assignmentnumber{2}
\def\assignmentdate{09-04-2012}
\def\githuburl{\small\url{https://github.com/alphabits/dtu-spring-2012/tree/master/02433/assignment-2}}
\def\githuburlfoot{\footnotesize\url{https://github.com/alphabits/dtu-spring-2012/tree/master/02433/assignment-2}}

\input{../../header}

%%% BEGIN DOCUMENT
\begin{document}

\maketitle

\noindent In this exercise two datasets containing noisy observations of log-population sizes are analyzed. The datasets are fitted to a theta logistic model for population growth and various types of decoding are performed. The theta logistic model can be written as a state-space model
%
\begin{align}
    P_t &= P_{t-1} + r_0 \left(1 - \left[\frac{\exp{(P_{t-1})}}{K}\right]^\theta\right) + e_t \label{eq:state-update} \\
    X_t &= P_t + u_t \label{eq:observation}
\end{align}
%
where $P_t$ is the log-population size and $e_t\sim N(0,Q)$, $u_t\sim N(0,R)$ are iid. and mutually independent. The constant $K$ is the log-population size when stability is reached. Equation (\ref{eq:state-update}) expresses the fact that whenever $P_t>\log(K)$ then $P_{t+1}<P_t$ with high probability. Therefore it is assumed that $K,\theta$ and $r_0$ are all positive. Since $Q$ and $R$ are variances they are also assumed positive. All five parameters are combined in the parameter vector $\myvec{\lambda} = (\theta, r_0, K, Q, R)$.

\section*{a) Plotting the data}
%
To start the analysis the two datasets are plotted and shown in figure~\ref{fig:dataplots}. From the figure it is seen that the two datasets have a very similar overall structure. For both dataset $P_1\approx 3$ and stationarity is reached around $K\approx\exp(7)\approx1000$.
%
\begin{figure}[tb]
    \centering
    \includegraphics[width=140mm]{\plotpath{dataset-1.pdf}}
    \includegraphics[width=140mm]{\plotpath{dataset-2.pdf}}
    \caption{Plot of the two datasets}
    \label{fig:dataplots}
\end{figure}
%

\section*{b) Approximating the SSM by a HMM}
%
By assuming that the state-space $P_t$ is bounded it can be discretizised and by this discretization the state-space model can be expressed as an Hidden markov model instead. 

For the state-space model given in equation \ref{eq:state-update} and \ref{eq:observation} it is now assumed that $P_t\in[2.1,8.4]$ and that the state-space is partitioned into $m=250$ intervals $\Omega_i=(b_{i-1},b_i)$ where $i=1,\dots,m$. The width of each interval is then given by
\begin{align*}
    w = \frac{8.4 - 2.1}{250} = 0.0252
\end{align*}
and the boundaries can be expressed as $b_i = 2.1 + wi$. If $P_t\in\Omega_i$ it is said to be in state $i$ which corresponds to the discrete state-space variable $C_t$ being equal to $i$. Each discrete state $i$ is represented by the midpoint $p_i$ of the interval $(b_{i-1},b_i)$ given by $p_i = 2.1 + w(i-0.5)$. This links the discrete, integer valued state-space $C_t$ to the original continous state-space $P_t$.

To express the state-space model as a Hidden markov model the state dependent distibutions should be found. The distribution of $X_t\given C_t=i$ is found by replacing $P_t$ with the representation point $p_i$ in equation (\ref{eq:observation}). Combined with $u_t\sim N(0,R)$ gives that 
%
\begin{equation*}
    X_t\given C_t=i\:\sim\: N(p_i, R)
\end{equation*}
%
or equivalently
%
\begin{equation*}
    p(x_t\given C_t=i) = \frac{1}{\sqrt{2\pi R}}\exp\left(-\frac{(x_t - p_i)^2}{2R}\right)
\end{equation*}

Apart from the state dependent distributions the transition probabilities $\Pr(P_t\in\Omega_j\given P_{t-1}\in\Omega_i)$ should also be found. Conditioning on $P_{t-1}$ being in state $i$ ($C_{t-1}=i$) can be represented by replacing $P_{t-1}$ with $p_i$ in equation \ref{eq:state-update}. Using the fact that $e_t\sim N(0,Q)$ then gives 
\begin{equation*}
    P_t \given C_{t-1}=i \:\sim\: N\left(\mu_i, Q\right) \quad\text{with}\quad \mu_i = p_i + r_0\left(1-\left[\frac{\exp(p_i)}{K}\right]^\theta\right)
\end{equation*}
%
Letting $n(\bullet, \mu, \sigma^2)$ denote the Gaussian pdf with mean $\mu$ and variance $\sigma^2$ the transition probabilities can then be calculated by
%
\begin{equation*}
    \Pr(P_t\in\Omega_j\given C_{t-1}=i) = \Pr(C_t=j\given C_{t-1}=i) = \int_{\Omega_j} n(p_t, \mu_i, Q)\, dp_t
\end{equation*}
%
This integral could be calculated by using the cumulative distribution function for the normal distribution, but as mentioned on page 13 in \citep{waever11} this is more computational expensive than approximating the integral using the trapezoidal rule for integration. Using the trapezoidal rule gives
%
\begin{equation*}
    \Pr(C_t = j\given C_{t-1}=i) \approx \frac{w}{2}\left(n(b_{j-1}, \mu_i, Q) + n(b_j, \mu_i, Q)\right)
\end{equation*}


\section*{c) Computing the likelihood}

In this section a function that evaluates the likelihood of the discretized state-space model is created. Since the discretized state-space model is a HMM the algorithm described on page 47 in \citep{zucchini09} can be used. To implement the algorithm a few details need to be handled.

The markov chain of the discretized SSM can not be assumed stationary so an initial state distribution is needed. This is done by using the approximation mentioned on the bottom of page 7 in \citep{waever11}.

Also the constraints on the parameters $\myvec{\lambda}=(\theta, r_0, K, Q, R)$ need to be handled. Since all parameters are constrained to be positive reals the logarithm of the natural parameters will be unconstrained which gives the working parameters
\begin{equation*}
    \myvec{\tau} = \log(\myvec{\lambda})
\end{equation*}
Going from working parameters to natural parameters is then
\begin{equation*}
    \myvec{\lambda} = \exp(\myvec{\tau})
\end{equation*}
(where $\log$ and $\exp$ is element-wise functions).

Finally the state dependent distributions are continous so if one of the means $\mu_i$ equals one of the observations $x_t$ and we then let $Q\to0$ the likelihood will be unbounded. The implementation in this section will not take care of this detail though.

\subsection*{Implementation}

The implementation of the log likelihood function is shown in \coderef{mllk}. The function takes the working parameters in a vector and returns the minus log likelihood value. This makes it easy to use the build-in unconstrained minization function \myverb{nlm} to maximize the likelihood. The implementation shown in \coderef{mllk} is taken a bit out of context. The function definition is part of the parent function \myverb{get.HMM.model} for which the source code is shown in \appref{modelfunction}. The reason that the \myverb{HMM.mllk} and \myverb{HMM.mle} functions are created by another function is to let them easily share variables (eg. the data \myverb{x}, the boundary points \myverb{b.i}, the representative points \myverb{p.i} etc.) without cluttering the global namespace. The concept that functions created within a parent function has access to the variables of the parent function -- even after the parent function has returned its value -- is called closure. 

\lstinputlisting[firstline=35,lastline=56,caption={Function that calculates the minus log likelihood of the discretized SSM. The function depends on variables defined in the closure of the function definition. See \appref{modelfunction} for the complete source code},label={src:mllk}]{\srcpath{model.R}}
%
The \myverb{HMM.mllk} function also uses a helper function \myverb{calc.gamma} to calculate the transition probability matrix. The \myverb{calc.gamma} function is also defined within the \myverb{get.HMM.model} function and therefore also have access to eg. the representative points \myverb{p.i}.

\lstinputlisting[firstline=18,lastline=33,caption={The helper function \myverb{calc.gamma} used in the \myverb{HMM.mllk} function}]{\srcpath{model.R}}

To actually calculate the minus log likelihood for a specific choice of parameters we call

\lstinputlisting[caption={Using the implementation in \coderef{mllk}}]{\srcpath{calculate-mllk.R}}
%
The function \myverb{get.HMM.model} is called with the data \myverb{dat1}, the number of intervals, and the state-space boundaries. The returned list contains all functions related to the model and these functions have access to the data due to the closure created by the \myverb{get.HMM.model} function.


\section*{d) Estimating parameters}

In this section a function to estimate the parameters of the model by maximum likelihood is implemented. The function will also return an estimate of the standard errors of the maximum likelihood estimates. The error estimates are calculated from the hessian of minus the log likelihood at the minimum using equation 3.2 in \citep{zucchini09} to express the hessian wrt. the natural parameters $\myvec{\lambda}$. The implementation is shown in \coderef{mle} and the function \myverb{HMM.mle} is also defined within the function \myverb{get.HMM.model} which gives it access to the data \myverb{x} due to the closure explained in the previous section.

\lstinputlisting[firstline=58,lastline=73,caption={Function to estimate the parameters by maximum likelihood},label={src:mle}]{\srcpath{model.R}}

The function \myverb{HMM.mle} uses the helper function \myverb{inv.hessian.from.working.hessian} to calculate an estimate of the variance-covariance matrix. The function \myverb{inv.hessian.from.working.hessian} is shown in \coderef{hessian}

\lstinputlisting[firstline=1,lastline=4,caption={Function to transform the hessian from working parameters to natural parameters},label={src:hessian}]{\srcpath{model.R}}

\subsection*{Estimating parameters for dataset 1}

To estimate the parameters of dataset 1 some initial parameters need to be determined. From figure~\ref{fig:dataplots} it seems that $K\approx e^{7}\approx1000$. The other parameters are a bit harder to determine but it seems plausible to keep $\theta$ and $r_0$ small (0.5-1). After some fiddling the starting parameters\footnote{To ensure that the maximum found isn't a local maximum much smaller than the global maximum, some other starting parameters was tried. They all gave the same maximum which indicates that the parameter estimates may be the maximum likelihood estimates.} was chosen as


\begin{equation*}
    \myvec{\lambda}_0 = \begin{pmatrix}
        0.5 & 0.2 & 1000 & 0.01 & 0.05
    \end{pmatrix}
\end{equation*}
%
Calculating the maximum likelihood gave the parameter estimates
%
\begin{equation*}
    \widehat{\myvec{\lambda}} = \begin{pmatrix}
        \input{\respath{cont-dataset-1-param-1-estimate-edit.tex}}
    \end{pmatrix}
\end{equation*}

with $-\ell = \input{\respath{cont-dataset-1-param-1-mllk}}$\footnote{The likelihood value at the maximum is very high. This is probably due to the fact that the state dependent probabilities are calculated as point values of the normal density function instead of ``real'' probabilities.} and $\text{AIC}=\input{\respath{cont-dataset-1-param-1-aic}}$.
%
Using the hessian returned from the \myverb{nlm} function an approximate variance-covariance matrix was calculated as
%
\begin{equation*}
    \widehat{\Var{\myvec{\widehat{\lambda}}}} = \begin{pmatrix}
        \input{\respath{cont-dataset-1-param-1-varcov-edit.tex}}
    \end{pmatrix}
\end{equation*}
%
From the approximate variance-covariance matrix the correlation of all pairs of parameter estimates can be calculated. The three numerically largest was
%
\begin{equation*}
    \Corr{\widehat{\theta}, \widehat{r_0}} = -0.954\quad\quad
    \text{and}\quad\quad
    \Corr{\widehat{Q}, \widehat{R}} = -0.315\quad\quad
    \text{and}\quad\quad
    \Corr{\widehat{\theta}, \widehat{Q}} = 0.247
\end{equation*}
%
The large negative correlation between $\widehat{\theta}$ and $\widehat{r_0}$ can be explained by looking at the expression
\begin{equation*}
    r_0\left(1 - \left[\frac{\exp{(P_{t-1})}}{K}\right]^\theta\right)
\end{equation*}
An increase in $\theta$ will always make the above expression numerically larger. To counteract this we can decrease $r_0$. Therefore datasets that makes $\widehat{\theta}>\E{\widehat{\theta}}$ will with high probability make $\widehat{r_0}<\E{\widehat{r_0}}$, since the above expression represents the expected change in population level and the true expected change in population level is independent of how we choose to parameterize our model.

That $\widehat{Q}$ and $\widehat{R}$ are negatively correlated can be explained by substituting equation (\ref{eq:state-update}) into equation (\ref{eq:observation}), which gives a term $u_t+e_t$ that is normal distributed with variance $Q+R$.

From the variance-covariance matrix confidence intervals for the parameter estimates can be calculated and the result is shown in table~\ref{tbl:params-dataset-1}. 

\begin{table}[htb]
    \centering
    \begin{tabular}{lr|rr|r}
         & \multicolumn{1}{r}{Estimate} & \multicolumn{2}{c}{95\% Conf.Int} & Std.Dev \\\hline
        \input{\respath{cont-dataset-1-param-1-confidence.tex}} \\
    \end{tabular}
    \caption{Parameter estimates with 95\% confidence intervals calculated from the inverse hessian at the maximum}
    \label{tbl:params-dataset-1}
\end{table}

It is seen that the confidence intervals for both $\widehat{\theta}$ and $\widehat{r_0}$ includes 0 and in general the standard deviation of the estimators are large relative to the corresponding estimates.


\subsection*{Estimating parameters for dataset 2}

From figure~\ref{fig:dataplots} is seems that dataset 2 is from a population that is similar to the population of dataset 1. Therefore the same initial parmeters are used

Calculating the maximum likelihood estimates gives the parameter estimate
%
\begin{equation*}
    \widehat{\myvec{\lambda}} = \begin{pmatrix}
        \input{\respath{cont-dataset-2-param-1-estimate-edit.tex}}
    \end{pmatrix}
\end{equation*}
%
with $-\ell = \input{\respath{cont-dataset-2-param-1-mllk}}$ and $\text{AIC}=\input{\respath{cont-dataset-2-param-1-aic}}$. Comparing the estimates for dataset 2 with the estimates for dataset 1 it is seen that most parameter estimates are almost equal. Only the estimate of $\theta$ differs; the estimate from dataset 2 is twice the size of the estimate from dataset 1, but notice that $\widehat{\theta}$ from dataset 2 is within the 95\% confidence interval of $\widehat{\theta}$ from dataset 1.

The approximate variance-covariance matrix is found as
%
\begin{equation*}
    \widehat{\Var{\myvec{\widehat{\lambda}}}} = \begin{pmatrix}
        \input{\respath{cont-dataset-2-param-1-varcov-edit.tex}}
    \end{pmatrix}
\end{equation*}
%
and the numerically largest correlations are found to be
%
\begin{equation*}
    \Corr{\widehat{\theta}, \widehat{r_0}} = -0.807\quad\quad
    \text{and}\quad\quad
    \Corr{\widehat{Q}, \widehat{R}} = -0.465\quad\quad
    \text{and}\quad\quad
    \Corr{\widehat{\theta}, \widehat{Q}} = 0.452
\end{equation*}
%
It is seen that it is the same pairs of estimators as for dataset 1.

From the variance-covariance matrix confidence intervals for the parameter estimates can be calculated and the result is shown in table~\ref{tbl:params-dataset-2}

\begin{table}[htb]
    \centering
    \begin{tabular}{lr|rr|r}
         & \multicolumn{1}{r}{Estimate} & \multicolumn{2}{c}{95\% Conf.Int} & Std.Dev \\\hline
        \input{\respath{cont-dataset-2-param-1-confidence.tex}} \\
    \end{tabular}
    \caption{Parameter estimates for dataset 2 with 95\% confidence intervals calculated from the inverse hessian at the maximum}
    \label{tbl:params-dataset-2}
\end{table}

It is seen that the 95\% confidence interval for $\theta$ still includes 0 but the standard errors for both $r_0$ and $K$ has decreased compared with the standard errors calculated from dataset 1.

\section*{e) Local decoding}

In this section local decoding for the two fitted HMMs will be calculated. The implementation is based on A.2.2, A.2.5 and A.2.6 in \citep{zucchini09} and can be seen in \appref{modelfunction}.
Using the implementation local decodings was calculated and plotted which is shown in figure~\ref{fig:decoding}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=140mm]{\plotpath{cont-dataset-1-param-1-decoding.pdf}}
    \includegraphics[width=140mm]{\plotpath{cont-dataset-2-param-1-decoding.pdf}}
    \caption{Local decoding for the two datasets using the parameters fitted by maximum likelihood}
    \label{fig:decoding}
\end{figure}

\FloatBarrier
\pagebreak

\renewcommand\thesection{\Alph{section}}
\section{Appendices}

All R code created for this assignment is included here. All source code incl. latex code for this report can be found at \githuburl

\subsection{Function creating model functions}
\label{app:modelfunction}
\lstinputlisting[firstline=6,lastline=138]{\srcpath{model.R}}

\subsection{Estimating parameters and decodings}
\label{app:estimation-decoding}
\lstinputlisting{\srcpath{calculate-mle.R}}

\FloatBarrier
\pagebreak

\subsection{Plotting data}
\label{app:plotdata}
\lstinputlisting{\srcpath{plotdata.R}}

\subsection{Helper functions}
\label{app:functions}
\lstinputlisting{\srcpath{functions.R}}

\FloatBarrier
\pagebreak

\nocite{zucchini09,waever11}
\bibliographystyle{plain}
\bibliography{../../../bibliography}

\end{document}
